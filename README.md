# 一、什么是人工智能

人工智能就其本质而言，是机器对人的息维或行为过程的模拟让它能像人一样思考或行动。根据输入信息进行模型结构、权重更新实现最终优化。特点:信息处理、自我学习、优化升级。

## 1.1 人工智能实现方法

- 机器学习（Machine Learning, ML）从数据中寻找规律、建立关系，根据建立的关系去解决问题的方法。从数据中学习并且实现自我优化与升级

  - 监督学习（Supervised Learning）：通过训练数据集（包含正确的结果）学习输入和输出之间的关系。
  - 无监督学习（Unsupervised Learning）：在没有标签的数据（包含不正确的结果）中寻找模式和结构。
  - 半监督学习（Semi-supervised Learning）：结合有标签和无标签的数据（少量的正确结果）进行学习。
  - 强化学习（Reinforcement Learning）：通过与环境的交互来学习如何做出决策。（根据结果收获的奖惩进行学习，如 AlphaGo）

- 深度学习（Deep Learning, DL）

  - 卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像识别和处理。
  - 循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据和时间序列分析。
  - 长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的 RNN，适合处理和预测时间序列数据。
  - 生成对抗网络（Generative Adversarial Networks, GANs）：通过生成器和判别器的对抗学习生成新的数据样本。

- 符号推理（Symbolic Reasoning）

  - 使用逻辑规则和符号操作来模拟人类推理过程。

- 进化算法（Evolutionary Algorithms）

  - 模仿自然选择和遗传机制来优化问题解决方案。

- 专家系统（Expert Systems）

  - 模拟专家的决策过程，通常基于规则引擎。根据既定逻辑和规则进行推理。无法处理新数据，无法学习新知识。

- 自然语言处理（Natural Language Processing, NLP）

  - 包括语言模型、情感分析、机器翻译等技术，使机器能够理解和生成自然语言。

## 1.2 机器学习和深度学习

- 包含关系：

  - 深度学习是机器学习的一个子集：深度学习是机器学习中的一种技术，专注于使用多层神经网络（即深度神经网络）来学习数据的复杂模式和特征。机器学习是一个更广泛的领域，包括了多种算法和方法，如决策树、支持向量机（SVM）、随机森林等。

- 数据和特征：

  - 机器学习：特征工程：在传统的机器学习中，通常需要人工提取特征，这需要领域知识和大量的工作。特征的好坏直接影响模型的性能。
  - 深度学习：自动特征提取：深度学习通过多层神经网络自动学习数据的高级特征，减少了对人工特征工程的依赖。

- 模型复杂度：

  - 机器学习：模型简单：传统的机器学习模型通常较为简单，易于理解和解释。
  - 深度学习：模型复杂：深度学习模型包含大量的参数和层，能够捕捉数据中的复杂和抽象的模式，但也更难解释。

- 数据需求：

  - 机器学习：数据量要求较低：传统的机器学习算法可以在较小的数据集上进行训练。
  - 深度学习：数据量要求较高：深度学习模型通常需要大量的数据来训练，以避免过拟合并提高性能。

- 计算资源：

  - 机器学习：计算资源要求较低：传统的机器学习算法可以在普通的计算机上运行。
  - 深度学习：计算资源要求较高：深度学习模型需要大量的计算资源，如 GPU，以加速训练过程。

- 应用领域：
  - 机器学习：广泛应用：机器学习技术被应用于各种领域，如金融、医疗、电商等。
  - 深度学习：特定领域：深度学习在图像识别、语音识别、自然语言处理等领域取得了显著的成果。

# 二、机器学习

y = 1000 \* 1.1^x

- 传统算法： 输入数据和算法，通过计算得到结果
- 机器学习：输入数据，计算机器通过学习得到算法，通过算法得到结果

## 2.1 什么是回归分析

回归分析是一种统计方法，用于研究因变量（目标变量）与一个或多个自变量（解释变量）之间的关系。回归分析的目标是建立一种数学模型，该模型可以预测因变量的值，基于自变量的值。

可以理解为数学中的模拟曲线，通过已知数据拟合出一条曲线（一个 y=f(x1,x2,x3...xn)），通过曲线预测未知数据

- 一元回归分析：只有一个自变量 y = f(x)
- 多元回归分析：有多个自变量 y = f(x1,x2,x3...xn)
- 线性回归分析：自变量和因变量之间的关系是线性的 y = ax + b
- 非线性回归分析：自变量和因变量之间的关系是非线性的 y = ax^2 + bx + c

## 2.2 监督学习之线性回归

目标值和特征值之前的差距尽可能小

M 是样本数量，y 是目标值，y'是预测值，1/2M 是权重， 2m 是方便求导，(y - y')^2 是误差

损失函数选择：min(1/2M \* ∑ \* ( y' - y )^2)

找到一个最合适的 a 和 b 使得损失函数最小

梯度下降法：通过不断调整 a 和 b 的值，使得损失函数的值逐渐减小，最终找到最优解。

梯度下降法的基本思想是：首先随机初始化模型参数，然后计算损失函数关于每个参数的梯度（即偏导数），接着沿着梯度的反方向（因为梯度指向最大增加的方向，所以要反方向）更新参数，以此来逐步逼近最小损失（找到损失函数的极小值）。（梯度越大说明 y 和 y'的误差越大）

min(1/2M \* ∑ \* ( y' - y )^2) = min(1/2M \* ∑ \* ( ax + b - y )^2) = f(a,b)

通过求偏导数，得到 a 和 b 的更新公式，然后不断迭代更新 a 和 b 的值，直到损失函数的值不再变化或者变化非常小。

### 2.2.1 步骤

1. 数据预处理：对数据进行清洗、归一化、特征选择等操作，以便于后续的模型训练。
2. 生成损失函数
3. 选择回归模型：选择适合的回归模型，如线性回归、多项式回归、岭回归等。
4. 训练模型：使用训练数据集对模型进行训练，通过梯度下降法等优化算法来调整模型的参数，使得损失函数的值最小。
5. 模型评估：使用测试数据集对模型进行评估，计算模型的准确率、MSE 均方误差，R 方值等指标，以评估模型的性能。
6. 模型预测：使用训练好的模型对新的数据进行预测，得到预测结果。

### 2.2.2 例子

- [单因子线性回归](/sl/line.py)
- [多音字线性回归](/sl/mutil.py)

## 2.3 监督学习之逻辑回归（分类问题）

在进行分类问题时吗，样板量变大后，使用线性回归模型，模型效果变差，可能的原因是什么？

1. 样本量变大后，模型复杂度增加，导致过拟合
2. 样本量变大后，模型训练时间增加，导致模型训练不稳定
3. 样本量变大后，模型训练数据分布发生变化，导致模型效果变差

二分类 f(x) = 1 / (1 + e^(-g(x))) = sigmoid(g(x))

- y = 1, sigmoid(g(x0)) > 0.5
- y = 0, sigmoid(g(x)) < 0.5

g(x) 可以是线性回归模型，也可以是多项式回归模型。 -g(x)也称为逻辑回归模型的决策函数(边界)。

损失函数选择

- -log(f(x)) 当 y = 1
- -log(1 - f(x)) 当 y = 0

则确定损失函数 J = -1/m \* ∑ \* ( y \* log(f(x)) + (1 - y) \* log(1 - f(x)) )

### 2.3.1 例子

- [逻辑回归](/sl/line.py)

## 2.4 无监督学习

机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的数据进行分类或分群。

优点:

- 算法不受监督信息(偏见)的约束，可能考虑到新的信息
- 不需要标签数据，极大程度扩大数据样本

主要应用:聚类分析、关联规则、维度缩减

### 2.4.1 无监督学习 vs 监督学习

| 特征     | 监督学习                                    | 无监督学习                                       |
| -------- | ------------------------------------------- | ------------------------------------------------ |
| 数据标签 | 训练数据包含标签                            | 训练数据不包含标签                               |
| 目标     | 预测或决定未见过的数据的标签                | 探索数据的内在结构和模式                         |
| 算法     | 线性回归、逻辑回归、决策树、SVM、神经网络等 | K-均值聚类、层次聚类、PCA、自编码器等            |
| 应用场景 | 分类（垃圾邮件识别）、回归（房价预测）      | 数据挖掘、异常检测、市场细分、社交网络分析       |
| 性能评估 | 准确率、召回率、F1 分数、均方误差等         | 聚类有效性指标（轮廓系数）、可视化方法等         |
| 可解释性 | 预测结果容易解释                            | 结果解释性较差，需要人为解读                     |
| 数据需求 | 需要大量标记数据                            | 不需要标记数据，适用于数据量大但标记成本高的场景 |

### 2.4.2 聚类分析

聚类分析是一种无监督学习方法，旨在将相似的数据点分组在一起，形成不同的簇。聚类分析的目标是找到数据的内在结构和模式，以便更好地理解数据。

### 2.4.3 KMean 聚类

以空间中的 k 个点为中心进行聚类，对最靠近的他们的点进行归类。如何更新中心点：将每个簇中所有点的均值作为新的中心点。直到中心点不再发生变化。

- 根据数据与中心点距离划分类别
- 基于类别数据更新中心点
- 重复过程直到收敛

优点

- 原理简单，实现容易，收敛速度快
- 参数少，方便使用

缺点:

- 必须设置簇的数量
- 随机选择初始聚类中心，结果可能缺乏一致性

### 2.4.4 均值漂移聚类 Mean Shift

均值漂移算法:一种基于密度梯度上升的聚类算法(沿着密度上升方向寻找聚类中心点)

- 根据数据与中心点距离划分类别
- 基于类别数据更新中心点
- 重复过程直到收敛

算法流程

1. 随机选择未分类点作为中心点
2. 找出离中心点距离在带宽之内的点，记做集合 SN m
3. 计算从中心点到集合 S 中每个元素的偏移问量 M
4. 中心点以向量 M 移动
5. 重复步骤 2-4，直到收敛
6. 重复 1-5 直到所有的点都被归类
7. 分类:根据每个类，对每个点的访问频率，取访问频率最大的那个类，作为当前点集的所属类

特点:

- 实现复杂，收敛慢
- 不需要指定类别数量，需要选择区域半径

### 2.4.5 k 近邻分类 KNN(监督学习)

给定一个训练数据集（已分类），对新的输入实例，在训练数据集中找到与该实例最邻近这 K 个实例的多数属于某个类，就的 K 个实例了也就是上面所说的 K 个邻居把该输入实例分类到这个类中。最简单的机器学习算法之一

### 2.4.6 KMeans vs KNN

K-Means

- 类型：无监督学习算法。
- 目的：用于聚类分析，即将数据划分为 K 个簇（类别），使得簇内的数据点尽可能相似，而簇与簇之间的数据点尽可能不相似。
- 输出：每个数据点被分配到一个簇中，最终得到 K 个簇的中心点（质心）。

KNN

- 类型：监督学习算法，既可以用于分类任务，也可以用于回归任务。
- 目的：用于预测新数据点的类别（分类）或值（回归）。通过查找训练集中与新数据点最相似的 K 个邻居，根据这些邻居的信息来预测新数据点的输出。
- 输出：对于分类任务，输出是新数据点的类别；对于回归任务，输出是新数据点的预测值。

### 2.4.7 DBSCAN 算法基于密度的空间聚类算法

- 基于区域点密度筛选有效数据
- 基于有效数据向周边扩张，直到没有新点加入

特点:

- 过滤噪音数据
- 不需要人为选择类别数量
- 数据密度不同时影响结果

### 2.4.8 例子

- [KMeans 聚类](/ul/kmeans.py)
- [KNN](/ul/knn.py)
- [MeanShift](/ul/meanshift.py)

无监督学习得出的分类，类别可能和源数据不一样，此时，需要矫正一下才能获得 1 的准确率

## 2.5 决策树 Decision Tree (监督)

一种对实例进行分类的树形结构,通过多层判断区分目标所属类别。本质:通过多层判断,从训练数据集中归纳出一组分类规则

优点:

- 计算量小,运算速度快
- 易于理解,可清晰查看各属性的重要性

缺点:

- 忽略属性间的相关性
- 样本类别分布不均匀时,容易影响模型表现

- 逻辑回归模型使用的是 一个线性回归模型，将特征值带入到 sigmoid 函数中，得到的值大于 0.5 则为 1，小于 0.5 则为 0
- 决策树模型使用的是 一个树形结构，将特征值带入到树中，根据特征值的大小，逐步判断，最终得到结果

### 2.5.1 决策树之 ID3

决策树的三种特征选择算法：ID3，C4.5，CART

ID3: 利用信息熵原理选择信息增益最大的属性作为分类属性,递归地招展决策树的分枝,完成决策树的构造

信息熵(entropy)是度量随机变量不确定性的指标,(熵越大)就越大。根据信息熵,可以计算以属性 a 进行样本划分带来的信息增益。

信息增益（Information Gain）是信息论中的一个概念，用于衡量通过知道一个随机变量的值而获得的关于另一个随机变量的信息量。在决策树学习中，信息增益是选择属性进行分裂的标准之一。信息增益的计算公式如下：

Gain(X,Y)=H(X)−H(X∣Y)

其中：

- H(X) 是随机变量 X 的信息熵。
- H(X∣Y) 是在已知随机变量 Y 的条件下，随机变量 X 的条件信息熵。
- 信息增益表示在知道 Y 的值之后，关于 X 的不确定性减少了多少。信息增益越大，表示 Y 对 X 的信息量越大，即 Y 能更好地预测 X。

目标:划分后样本分布不确定性尽可能小,即划分后信息熵小,信息增益大。判断每个属性的信息增益

｜｜动力｜能力｜兴趣｜时间｜｜-｜-｜-｜-｜-｜｜信息熵｜ 0.6|0.55|0.36|0.55| ｜信息增益｜ 0.37|0.42|0.61|0.42|

兴趣的信息增益最大，所以兴趣作为决策树的第一层判断

### 2.5.2 例子

- [决策树](/iris/dt.py)

## 2.6 异常检测 Anomaly Detection (无监督)

根据输入数据，对不符合预期模式的数据进行识别

- 概率密度函数：
- 高斯分布
- 数据均值
- 标准差

当维度大于一时，使用每个维度的高斯分布函数乘积 p(x) = p(x1) _ p(x2) _ ... p(xn)

### 2.6.1 例子

- [异常检测](/iris/noise.py)

## 2.7 主成分分析 PCA

### 数据降维

数据降维,是指在某些限定条件下,降低随机变量个数,得到到一组"不相关"主变量的过程。

作用:

- 减少模型分析数据量,提升处理效率,降低计算难度,
- 实现数据可视化。

PCA 是数据降维最常用的方法。

- 如何保留主要信息:投影后的不同特征数据尽可能分得开，即不相关

如何实现? 使投影后数据的方差最大,因为方差越大数据也越分散

计算过程:

- 原始数据预处理(标准化:u=0,σ=1)
- 计算协方差矩阵特征向量、及数据在各特征向量投影后的方差
- 根据需求(任务指定或方差比例)确定降维维度 k
- 选取 k 维特征向量,计算数据在其形成空间的投影

### PCA vs 线性回归

| **方面** | **PCA（主成分分析）** | **线性回归** |
| --- | --- | --- |
| **目标** | 降维，简化数据结构，保留最大方差信息。 | 预测，建立自变量与因变量之间的线性关系。 |
| **输入** | 高维数据矩阵 \(X\)。 | 自变量矩阵 \(X\) 和因变量向量 \(y\)。 |
| **输出** | 降维后的数据、主成分（特征向量）。 | 回归系数 \( \beta \)。 |
| **数学原理** | 特征值分解（Eigendecomposition）或奇异值分解（SVD）；基于协方差矩阵。 | 最小二乘法；最小化误差平方和 \( \| y - X\beta \|^2 \)。 |
| **求解方法** | 1. 数据标准化<br>2. 计算协方差矩阵<br>3. 特征分解<br>4. 选择主成分<br>5. 投影数据。 | 1. 模型设定 \( y = X\beta + \epsilon \)<br>2. 最小二乘法<br>3. 解析解 \( \beta = (X^TX)^{-1}X^Ty \)。 |
| **数据要求** | 需要对数据进行标准化（均值为 0，方差为 1），因为 PCA 对数据尺度敏感。 | 通常不需要标准化，但标准化可以提高数值稳定性。 |
| **应用场景** | 降维、特征提取、数据可视化、去除噪声、探索数据结构。 | 预测、因果分析、建立变量关系、回归分析。 |
| **对误差的处理** | 不涉及误差最小化，而是最大化数据的方差保留。 | 最小化预测值与实际值之间的误差平方和。 |
| **是否需要标签** | 无监督学习，不需要标签数据。 | 有监督学习，需要因变量 \( y \) 作为标签。 |

### 2.7.1 例子

- [异常检测](/iris/pca.py)

## 2.8 模型评价和优化

### 2.8.1 欠拟合和过拟合

在拟合曲线（数据是二次曲线）， 如果刚开始使用的一次函数则是欠拟合，如果使用四次函数则是过拟合。欠拟合和过拟合都是模型表现不好的表现。

欠拟合是指模型在训练数据上的拟合效果不佳，无法准确捕捉数据中的规律。换句话说，模型过于简单，无法学习到数据中的复杂模式。

解决方法

- 增加模型复杂度：例如，使用更高阶的多项式回归、更复杂的机器学习算法（如决策树、神经网络）。
- 增加特征数量：引入更多的特征或构造新的特征，以帮助模型更好地捕捉数据规律。
- 减少正则化强度：如果模型受到过强的正则化约束，可能会导致欠拟合。
- 增加训练时间：有时模型需要更长时间的训练才能收敛。

过拟合是指模型在训练数据上表现得非常好，但在新的测试数据上表现不佳。这是因为模型过于复杂，学习到了训练数据中的噪声和细节，而无法泛化到新的数据。

解决方法

- 减少模型复杂度：例如，降低多项式的阶数（如）、减少神经网络的层数或节点数量。
- 增加正则化：通过 L1 或 L2 正则化约束模型的复杂度，防止过拟合。
- 增加训练数据：更多的数据可以帮助模型学习到更一般的规律，减少对噪声的依赖。
- 使用交叉验证：通过交叉验证评估模型的泛化能力，避免模型只在特定训练集上表现良好。
- 剪枝（对于决策树）：减少决策树的深度或叶节点数量，避免过度拟合训练数据。

### 2.8.2 数据分离

将数据集分为训练集和测试集，训练集用于训练模型，测试集用于评估模型的性能。通常，将数据集分为 70% 训练集和 30% 测试集。

### 2.8.3 混淆矩阵

分类任务中，计算测试数据集预测准确率(accuracy)以评估模型表现，局限性: 无法真实反映模型针对各个分类的预测准确度

对于 900 个 1，100 个 0，

- 模型 1 预测 850 个 1，50 个 0，准确率为 90%
- 模型 2 预测 900 个 1，0 个 0，准确率为 90%

准确率(accuracy)

- 没有体现数据预测的实际分布情况(0、1 本身的分布比例)
- 没有体现模型错误预测的类型

混淆矩阵，又称误差矩阵，用于描述分类模型预测结果与实际结果之间的对应关系。它是一个表格，包含四个主要指标：真阳性（TP）、真阴性（TN）、假阳性（FP）和假阴性（FN）。

- 真阳性（TP）：被模型正确预测为正类的样本数量。（实际为 1，预测为 1）
- 真阴性（TN）：被模型正确预测为负类的样本数量。（实际为 0，预测为 1）
- 假阳性（FP）：被模型错误预测为正类的负类样本数量。（实际为 1，预测为 0）
- 假阴性（FN）：被模型错误预测为负类的正类样本数量。（实际为 0，预测为 1）

混淆矩阵的四个指标可以用来计算各种评估指标，如准确率（accuracy）、精确率（precision）、召回率（recall）和 F1 分数（F1-score）等等。

### 2.8.4 模型优化

数据质量决定模型表现的上限，模型优化是提高模型表现的下限。

Always check:

- 数据属性的意义，是否为无关数据
- 不同属性数据的数量级差异性如何
- 是否有异常数据
- 采集数据的方法是否合理，采集到的数据是否有代表性
- 对于标签结果，要确保标签判定规则的一致性(统一标准)

| Always try                   | Benefits                   |
| ---------------------------- | -------------------------- |
| 删除不必要的属性             | 减少过拟合、节约运算时间   |
| 数据预处理:归一化、标准化    | 平衡数据影响，加快训练收敛 |
| 保留或过滤掉异常数据         | 提高鲁棒性                 |
| 尝试不同的模型，对比模型表现 | 帮助确定更合适的模型       |

### 2.8.5 例子

- [酶活性](/improve/enzyme.py)
- [质量检测](/improve/quality.py)

# 三、深度学习

## 3.1 多层感知器(MLP)

对于复杂的分类问题，如边界函数是个不规则的圈时，使用逻辑回归，那么得到的函数会非常复杂。同时如果初始的数据项属性非常多，再生成多项式数据的时候，数据量就会很庞大，使得建模速度特别慢。（100 项属性，多项式就超过 5000）。在处理图片视频等数据时，使用逻辑回归明显是不合适的.

### 3.1.1 神经网络分类和逻辑回归分类得区别

| **特性** | **逻辑回归分类** | **神经网络分类** |
| --- | --- | --- | --- |
| **基本原理** | 基于逻辑函数（Sigmoid）将线性组合的输入映射到二分类输出（0 或 1）。 | 基于多层神经元结构，通过激活函数和权重调整实现复杂的非线性分类。 |
| **模型结构** | 线性模型，只有一个输出层（Sigmoid 激活函数）。 | 非线性模型，包含输入层、隐藏层（可多个）和输出层，每层有多个神经元。 |
| **数学表达** | \( P(y=1 | x) = \frac{1}{1 + e^{-(w \cdot x + b)}} \) | \( y = f(W_1 \cdot f(W_2 \cdot f(...W_n \cdot x + b_n)...) + b_1) \) |
| **适用场景** | 适用于线性可分的数据集，适合简单的二分类问题。 | 适用于复杂数据集（线性不可分），支持多分类和复杂的模式识别。 |
| **训练复杂度** | 训练简单，通常使用梯度下降法优化损失函数（如交叉熵）。 | 训练复杂，需要反向传播算法优化多层权重，计算成本较高。 |
| **参数数量** | 参数较少，通常只有权重和偏置。 | 参数较多，尤其是深层网络，包含大量权重和偏置。 |
| **过拟合风险** | 由于模型简单，过拟合风险较低。 | 由于模型复杂，过拟合风险较高，需要正则化等技术来控制。 |
| **预测能力** | 预测能力有限，适合简单问题。 | 预测能力强，可以学习复杂的数据模式。 |
| **可解释性** | 模型可解释性强，权重可以直接反映特征的重要性。 | 模型可解释性差，尤其是深层网络，被称为“黑箱模型”。 |
| **扩展性** | 扩展性有限，难以处理复杂数据。 | 可扩展性强，可以通过增加层数和神经元数量提升性能。 |
| **实现难度** | 实现简单，大多数机器学习库都提供了现成的逻辑回归实现。 | 实现复杂，但现代深度学习框架（如 TensorFlow、PyTorch）简化了开发过程。 |
| **应用场景示例** | 医疗诊断（是否患病）、垃圾邮件检测（是否为垃圾邮件）。 | 图像识别（手写数字识别、人脸识别）、自然语言处理（情感分析、机器翻译）。 |

总结

- 逻辑回归：适合简单的二分类问题，模型简单、可解释性强，但预测能力有限。
- 神经网络：适合复杂数据和多分类问题，具有强大的预测能力和扩展性，但模型复杂、训练成本高、可解释性差。

神经元

- 树突的概念
- 轴突的概念

多层感知模型框架, 每个神经元都会由上一层的所有神经元经过计算得到 a1 = g(θ1 _ x1 + θ2 _ x2 + θ3 \* x3)

x1 a1 b1

x1 a2 b2

x3 a3 b3 y 输入神经元 隐藏层 隐藏层 输出层

### 3.1.2 MLP 实现非线形回归

### 3.1.3 多分类

多分类就是生成多个 y，取 y 概率高的

y = {1,2,3} 处理成

y = { [1,0,0], [0,1,0], [0,0,1]}

### 3.3.4 MLP 图像多分类的流程？

神经网络开发的应用接口

### 3.1.5 Keras 介绍

- 集成了深度学习中各类成熟的算法
- 能够以 Tensorflow 或者 Theano 作为后端运行

### 3.1.6 Keras 和 Tensorflow 的区别

| **特性** | **Keras** | **TensorFlow** |
| --- | --- | --- |
| **定位** | 高级神经网络 API，专注于快速实验和模型构建。 | 低级深度学习框架，支持广泛的计算任务，包括深度学习、强化学习等。 |
| **抽象层次** | 高级抽象，隐藏底层细节，代码简洁。 | 低级抽象，提供底层接口，代码复杂度较高（TensorFlow 1.x），2.x 提升了易用性。 |
| **易用性** | 易于上手，适合初学者和快速开发。 | 上手难度较高（1.x），但 2.x 通过引入 Keras 等改进了易用性。 |
| **灵活性** | 灵活性有限，适合标准模型。 | 灵活性极高，支持自定义操作、计算图优化等。 |
| **性能** | 性能依赖后端引擎（如 TensorFlow）。 | 提供丰富的性能优化工具，如 GPU 加速、分布式训练等。 |
| **社区和生态** | 社区活跃，有大量的教程和预训练模型。 | 生态丰富，支持多种语言，有大量工具（如 TensorFlow Serving、TensorFlow Lite）。 |
| **开发语言** | 主要使用 Python。 | 主要使用 Python，但支持多种语言（C++、Java 等）。 |
| **集成关系** | Keras 可以独立使用，也可以作为 TensorFlow 的子模块（`tf.keras`）。 | TensorFlow 2.x 集成了 Keras，推荐使用`tf.keras`。 |
| **适用用户** | 适合初学者、数据科学家和希望快速实验的开发者。 | 适合需要底层控制、性能优化和高级功能的开发者和研究人员。 |
| **代码示例** | `python<br>from keras.models import Sequential<br>model = Sequential()<br>model.add(Dense(64, activation='relu'))<br>model.compile(optimizer='adam', loss='mse')<br>` | `python<br>import tensorflow as tf<br>x = tf.placeholder(tf.float32)<br>y = tf.matmul(x, x)<br>with tf.Session() as sess:<br> print(sess.run(y, feed_dict={x: [[2, 3], [4, 5]]}))<br>` |
| **版本更新** | 独立版本更新较慢，但作为`tf.keras`后更新与 TensorFlow 同步。 | 持续更新，TensorFlow 2.x 与 1.x 有较大变化，2.x 更易用。 |

- loss 的概念 使用什么损失函数
- optimizer 的概 使用什么优化器，和 θ 有关

### 4.1.7 实战 使用 MLP 实现非线性分类

边界是一个十字，左上和右下是一类，坐下和右上是一类

### 3.1.8 实战 图像多分类

使用 mnist 数据集分类数字的图片

### 3.1.9 神经网络类型

前馈神经网络（Feedforward Neural Networks, FNN） （MLP 属于这种，最简单的深度学习模型之一）

特点：

- 最简单的神经网络类型，信息从输入层流向输出层，没有反馈连接。
- 通常包含多个隐藏层，可以学习数据中的复杂模式。应用场景：
- 分类任务（如手写数字识别）。
- 回归任务（如房价预测）。

卷积神经网络（Convolutional Neural Networks, CNN）

特点：

- 专门用于处理具有网格结构的数据（如图像）。
- 使用卷积层和池化层来提取局部特征，能够自动学习图像中的层次化特征。

应用场景：

- 图像分类（如 ImageNet）。
- 目标检测（如 YOLO、SSD）。
- 图像分割（如 U-Net）。

循环神经网络（Recurrent Neural Networks, RNN）

特点：

- 适合处理序列数据，具有记忆功能，能够利用之前的信息影响当前的输出。
- 常见的变体包括长短期记忆网络（LSTM）和门控循环单元（GRU），用于解决梯度消失问题。

应用场景：

- 语言建模（如预测下一个单词）。
- 机器翻译（如将一种语言翻译成另一种语言）。
- 时间序列预测（如股票价格预测）。

生成对抗网络（Generative Adversarial Networks, GAN）

特点：

- 包含生成器（Generator）和判别器（Discriminator），通过对抗训练生成新的数据样本。
- 生成器生成假数据，判别器区分真假数据，两者相互竞争，最终生成器能够生成逼真的数据。

应用场景：

- 图像生成（如生成人脸图像）。
- 数据增强（如生成新的训练样本）。
- 超分辨率（如将低分辨率图像提升为高分辨率）。

自编码器（Autoencoders）

特点：

- 一种无监督学习模型，用于数据压缩和降维。
- 包含编码器（将输入压缩为低维表示）和解码器（从低维表示重建输入）。
- 变体包括稀疏自编码器、去噪自编码器和变分自编码器（VAE）。

应用场景：

- 数据降维（如 PCA 的替代方法）。
- 异常检测（通过重建误差检测异常数据）。
- 图像去噪（如去除图像中的噪声）。

Transformer 网络

特点：

- 基于自注意力机制（Self-Attention），能够处理长距离依赖关系。
- 不依赖于循环结构，训练速度更快。

应用场景：

- 自然语言处理（如 BERT、GPT）。
- 图像处理（如 Vision Transformer, ViT）。
- 多模态任务（如结合文本和图像）。

图神经网络（Graph Neural Networks, GNN）

特点：

- 用于处理图结构数据，能够学习节点、边和图的表示。
- 通过聚合邻居信息来更新节点的特征。

应用场景：

- 社交网络分析（如预测用户关系）。
- 分子结构预测（如药物发现）。
- 推荐系统（如商品推荐）。

深度强化学习网络

特点：

- 结合深度学习和强化学习，通过与环境的交互学习最优策略。
- 常见的网络包括 Q-Network 和策略网络。

应用场景：

- 游戏（如 AlphaGo）。
- 机器人控制（如路径规划）。
- 自动驾驶（如车辆控制）。

混合神经网络

特点：

- 结合多种神经网络类型，以利用不同网络的优势。
- 例如，CNN + RNN 用于图像字幕生成（先用 CNN 提取图像特征，再用 RNN 生成描述）。

应用场景：

- 多模态任务（如图像和文本结合）。
- 复杂任务（如语音识别和翻译）。

## 3.2 卷积神经网络

更好的使用 MLP

## 3.3 循环神经网络

vue data 的 dep 里是组件的 watcher， 他负责通知何时更新，更不是告诉自己变化了什么，因为要考虑到属性可能会有依赖，如果里面还有个计算数学，如果不是组件 watcher 就会更新两次，有 diff 就 patch 一次, 而且如果每个属性都有一个渲染 watcher, 占内存，而且自己渲染自己，性能差，diff 明显性能更哈。也可以结合事件委托的思想。 diff 可以合并多次数据变更，还可以可以复用

同时可以参考这个文章：https://juejin.cn/post/7472291842873917467

响应对象负责的是 什么时候更新？

而虚拟 DOM 负责的是 如何高效的更新？

而为什么不直接操作 DOM？

- 更新粒度难以控制：每次数据变化都直接操作 DOM，可能引发多次重排（Reflow）和重绘（Repaint）。
- 代码复杂度高：需要手动管理 DOM 的增删改查，容易出错。
- 性能不可预测：对于复杂组件，频繁的细粒度 DOM 操作可能比批量的 Diff 更慢。

总结：响应式与 Diff 的关系

- 响应式系统：负责 “感知变化”（数据到组件的映射）。
- 虚拟 DOM + Diff：负责 “高效更新”（组件到 DOM 的映射）。

两者结合，既保证了开发的便捷性（自动响应数据变化），又保证了性能（最小化 DOM 操作）。这是 Vue 在开发体验和运行效率之间的完美平衡。
