# 一、什么是人工智能

人工智能就其本质而言，是机器对人的息维或行为过程的模拟让它能像人一样思考或行动。根据输入信息进行模型结构、权重更新实现最终优化。特点:信息处理、自我学习、优化升级。

## 1.1 人工智能实现方法

- 机器学习（Machine Learning, ML）从数据中寻找规律、建立关系，根据建立的关系去解决问题的方法。从数据中学习并且实现自我优化与升级

  - 监督学习（Supervised Learning）：通过训练数据集（包含正确的结果）学习输入和输出之间的关系。
  - 无监督学习（Unsupervised Learning）：在没有标签的数据（包含不正确的结果）中寻找模式和结构。
  - 半监督学习（Semi-supervised Learning）：结合有标签和无标签的数据（少量的正确结果）进行学习。
  - 强化学习（Reinforcement Learning）：通过与环境的交互来学习如何做出决策。（根据结果收获的奖惩进行学习，如 AlphaGo）

- 深度学习（Deep Learning, DL）

  - 卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像识别和处理。
  - 循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据和时间序列分析。
  - 长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的 RNN，适合处理和预测时间序列数据。
  - 生成对抗网络（Generative Adversarial Networks, GANs）：通过生成器和判别器的对抗学习生成新的数据样本。

- 符号推理（Symbolic Reasoning）

  - 使用逻辑规则和符号操作来模拟人类推理过程。

- 进化算法（Evolutionary Algorithms）

  - 模仿自然选择和遗传机制来优化问题解决方案。

- 专家系统（Expert Systems）

  - 模拟专家的决策过程，通常基于规则引擎。根据既定逻辑和规则进行推理。无法处理新数据，无法学习新知识。

- 自然语言处理（Natural Language Processing, NLP）

  - 包括语言模型、情感分析、机器翻译等技术，使机器能够理解和生成自然语言。

## 1.2 机器学习和深度学习

- 包含关系：

  - 深度学习是机器学习的一个子集：深度学习是机器学习中的一种技术，专注于使用多层神经网络（即深度神经网络）来学习数据的复杂模式和特征。机器学习是一个更广泛的领域，包括了多种算法和方法，如决策树、支持向量机（SVM）、随机森林等。

- 数据和特征：

  - 机器学习：特征工程：在传统的机器学习中，通常需要人工提取特征，这需要领域知识和大量的工作。特征的好坏直接影响模型的性能。
  - 深度学习：自动特征提取：深度学习通过多层神经网络自动学习数据的高级特征，减少了对人工特征工程的依赖。

- 模型复杂度：

  - 机器学习：模型简单：传统的机器学习模型通常较为简单，易于理解和解释。
  - 深度学习：模型复杂：深度学习模型包含大量的参数和层，能够捕捉数据中的复杂和抽象的模式，但也更难解释。

- 数据需求：

  - 机器学习：数据量要求较低：传统的机器学习算法可以在较小的数据集上进行训练。
  - 深度学习：数据量要求较高：深度学习模型通常需要大量的数据来训练，以避免过拟合并提高性能。

- 计算资源：

  - 机器学习：计算资源要求较低：传统的机器学习算法可以在普通的计算机上运行。
  - 深度学习：计算资源要求较高：深度学习模型需要大量的计算资源，如 GPU，以加速训练过程。

- 应用领域：
  - 机器学习：广泛应用：机器学习技术被应用于各种领域，如金融、医疗、电商等。
  - 深度学习：特定领域：深度学习在图像识别、语音识别、自然语言处理等领域取得了显著的成果。

# 二、机器学习

y = 1000 \* 1.1^x

- 传统算法： 输入数据和算法，通过计算得到结果
- 机器学习：输入数据，计算机器通过学习得到算法，通过算法得到结果

## 2.1 什么是回归分析

回归分析是一种统计方法，用于研究因变量（目标变量）与一个或多个自变量（解释变量）之间的关系。回归分析的目标是建立一种数学模型，该模型可以预测因变量的值，基于自变量的值。

可以理解为数学中的模拟曲线，通过已知数据拟合出一条曲线（一个 y=f(x1,x2,x3...xn)），通过曲线预测未知数据

- 一元回归分析：只有一个自变量 y = f(x)
- 多元回归分析：有多个自变量 y = f(x1,x2,x3...xn)
- 线性回归分析：自变量和因变量之间的关系是线性的 y = ax + b
- 非线性回归分析：自变量和因变量之间的关系是非线性的 y = ax^2 + bx + c

## 2.2 监督学习之线性回归

目标值和特征值之前的差距尽可能小

M 是样本数量，y 是目标值，y'是预测值，1/2M 是权重， 2m 是方便求导，(y - y')^2 是误差

损失函数选择：min(1/2M \* ∑ \* ( y' - y )^2)

找到一个最合适的 a 和 b 使得损失函数最小

梯度下降法：通过不断调整 a 和 b 的值，使得损失函数的值逐渐减小，最终找到最优解。

梯度下降法的基本思想是：首先随机初始化模型参数，然后计算损失函数关于每个参数的梯度（即偏导数），接着沿着梯度的反方向（因为梯度指向最大增加的方向，所以要反方向）更新参数，以此来逐步逼近最小损失（找到损失函数的极小值）。（梯度越大说明 y 和 y'的误差越大）

min(1/2M \* ∑ \* ( y' - y )^2) = min(1/2M \* ∑ \* ( ax + b - y )^2) = f(a,b)

通过求偏导数，得到 a 和 b 的更新公式，然后不断迭代更新 a 和 b 的值，直到损失函数的值不再变化或者变化非常小。

### 2.2.1 步骤

1. 数据预处理：对数据进行清洗、归一化、特征选择等操作，以便于后续的模型训练。
2. 生成损失函数
3. 选择回归模型：选择适合的回归模型，如线性回归、多项式回归、岭回归等。
4. 训练模型：使用训练数据集对模型进行训练，通过梯度下降法等优化算法来调整模型的参数，使得损失函数的值最小。
5. 模型评估：使用测试数据集对模型进行评估，计算模型的准确率、MSE 均方误差，R 方值等指标，以评估模型的性能。
6. 模型预测：使用训练好的模型对新的数据进行预测，得到预测结果。

### 2.2.2 例子

- [单因子线性回归](/sl/line.py)
- [多音字线性回归](/sl/mutil.py)

## 2.3 监督学习之逻辑回归（分类问题）

在进行分类问题时吗，样板量变大后，使用线性回归模型，模型效果变差，可能的原因是什么？

1. 样本量变大后，模型复杂度增加，导致过拟合
2. 样本量变大后，模型训练时间增加，导致模型训练不稳定
3. 样本量变大后，模型训练数据分布发生变化，导致模型效果变差

二分类 f(x) = 1 / (1 + e^(-g(x))) = sigmoid(g(x))

- y = 1, sigmoid(g(x0)) > 0.5
- y = 0, sigmoid(g(x)) < 0.5

g(x) 可以是线性回归模型，也可以是多项式回归模型。 -g(x)也称为逻辑回归模型的决策函数(边界)。

损失函数选择

- -log(f(x)) 当 y = 1
- -log(1 - f(x)) 当 y = 0

则确定损失函数 J = -1/m \* ∑ \* ( y \* log(f(x)) + (1 - y) \* log(1 - f(x)) )

### 2.3.1 例子

- [逻辑回归](/sl/line.py)

## 2.4 无监督学习

机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的数据进行分类或分群。

优点:

- 算法不受监督信息(偏见)的约束，可能考虑到新的信息
- 不需要标签数据，极大程度扩大数据样本

主要应用:聚类分析、关联规则、维度缩减

### 2.4.1 无监督学习 vs 监督学习

| 特征     | 监督学习                                    | 无监督学习                                       |
| -------- | ------------------------------------------- | ------------------------------------------------ |
| 数据标签 | 训练数据包含标签                            | 训练数据不包含标签                               |
| 目标     | 预测或决定未见过的数据的标签                | 探索数据的内在结构和模式                         |
| 算法     | 线性回归、逻辑回归、决策树、SVM、神经网络等 | K-均值聚类、层次聚类、PCA、自编码器等            |
| 应用场景 | 分类（垃圾邮件识别）、回归（房价预测）      | 数据挖掘、异常检测、市场细分、社交网络分析       |
| 性能评估 | 准确率、召回率、F1 分数、均方误差等         | 聚类有效性指标（轮廓系数）、可视化方法等         |
| 可解释性 | 预测结果容易解释                            | 结果解释性较差，需要人为解读                     |
| 数据需求 | 需要大量标记数据                            | 不需要标记数据，适用于数据量大但标记成本高的场景 |

### 2.4.2 聚类分析

聚类分析是一种无监督学习方法，旨在将相似的数据点分组在一起，形成不同的簇。聚类分析的目标是找到数据的内在结构和模式，以便更好地理解数据。

### 2.4.3 KMean 聚类

以空间中的 k 个点为中心进行聚类，对最靠近的他们的点进行归类。如何更新中心点：将每个簇中所有点的均值作为新的中心点。直到中心点不再发生变化。

- 根据数据与中心点距离划分类别
- 基于类别数据更新中心点
- 重复过程直到收敛

优点

- 原理简单，实现容易，收敛速度快
- 参数少，方便使用

缺点:

- 必须设置簇的数量
- 随机选择初始聚类中心，结果可能缺乏一致性

### 2.4.4 均值漂移聚类 Mean Shift

均值漂移算法:一种基于密度梯度上升的聚类算法(沿着密度上升方向寻找聚类中心点)

- 根据数据与中心点距离划分类别
- 基于类别数据更新中心点
- 重复过程直到收敛

算法流程

1. 随机选择未分类点作为中心点
2. 找出离中心点距离在带宽之内的点，记做集合 SN m
3. 计算从中心点到集合 S 中每个元素的偏移问量 M
4. 中心点以向量 M 移动
5. 重复步骤 2-4，直到收敛
6. 重复 1-5 直到所有的点都被归类
7. 分类:根据每个类，对每个点的访问频率，取访问频率最大的那个类，作为当前点集的所属类

特点:

- 实现复杂，收敛慢
- 不需要指定类别数量，需要选择区域半径

### 2.4.5 k 近邻分类 KNN(监督学习)

给定一个训练数据集（已分类），对新的输入实例，在训练数据集中找到与该实例最邻近这 K 个实例的多数属于某个类，就的 K 个实例了也就是上面所说的 K 个邻居把该输入实例分类到这个类中。最简单的机器学习算法之一

### 2.4.6 KMeans vs KNN

K-Means

- 类型：无监督学习算法。
- 目的：用于聚类分析，即将数据划分为 K 个簇（类别），使得簇内的数据点尽可能相似，而簇与簇之间的数据点尽可能不相似。
- 输出：每个数据点被分配到一个簇中，最终得到 K 个簇的中心点（质心）。

KNN

- 类型：监督学习算法，既可以用于分类任务，也可以用于回归任务。
- 目的：用于预测新数据点的类别（分类）或值（回归）。通过查找训练集中与新数据点最相似的 K 个邻居，根据这些邻居的信息来预测新数据点的输出。
- 输出：对于分类任务，输出是新数据点的类别；对于回归任务，输出是新数据点的预测值。

### 2.4.7 DBSCAN 算法基于密度的空间聚类算法

- 基于区域点密度筛选有效数据
- 基于有效数据向周边扩张，直到没有新点加入

特点:

- 过滤噪音数据
- 不需要人为选择类别数量
- 数据密度不同时影响结果

### 2.4.8 例子

- [KMeans 聚类](/ul/kmeans.py)
- [KNN](/ul/knn.py)
- [MeanShift](/ul/meanshift.py)

无监督学习得出的分类，类别可能和源数据不一样，此时，需要矫正一下才能获得 1 的准确率

## 2.5 决策树 Decision Tree (监督)

一种对实例进行分类的树形结构,通过多层判断区分目标所属类别。本质:通过多层判断,从训练数据集中归纳出一组分类规则

优点:

- 计算量小,运算速度快
- 易于理解,可清晰查看各属性的重要性

缺点:

- 忽略属性间的相关性
- 样本类别分布不均匀时,容易影响模型表现

- 逻辑回归模型使用的是 一个线性回归模型，将特征值带入到 sigmoid 函数中，得到的值大于 0.5 则为 1，小于 0.5 则为 0
- 决策树模型使用的是 一个树形结构，将特征值带入到树中，根据特征值的大小，逐步判断，最终得到结果

### 2.5.1 决策树之 ID3

决策树的三种特征选择算法：ID3，C4.5，CART

ID3: 利用信息熵原理选择信息增益最大的属性作为分类属性,递归地招展决策树的分枝,完成决策树的构造

信息熵(entropy)是度量随机变量不确定性的指标,(熵越大)就越大。根据信息熵,可以计算以属性 a 进行样本划分带来的信息增益。

信息增益（Information Gain）是信息论中的一个概念，用于衡量通过知道一个随机变量的值而获得的关于另一个随机变量的信息量。在决策树学习中，信息增益是选择属性进行分裂的标准之一。信息增益的计算公式如下：

Gain(X,Y)=H(X)−H(X∣Y)

其中：

- H(X) 是随机变量 X 的信息熵。
- H(X∣Y) 是在已知随机变量 Y 的条件下，随机变量 X 的条件信息熵。
- 信息增益表示在知道 Y 的值之后，关于 X 的不确定性减少了多少。信息增益越大，表示 Y 对 X 的信息量越大，即 Y 能更好地预测 X。

目标:划分后样本分布不确定性尽可能小,即划分后信息熵小,信息增益大。判断每个属性的信息增益

｜｜动力｜能力｜兴趣｜时间｜｜-｜-｜-｜-｜-｜｜信息熵｜ 0.6|0.55|0.36|0.55| ｜信息增益｜ 0.37|0.42|0.61|0.42|

兴趣的信息增益最大，所以兴趣作为决策树的第一层判断

### 2.5.2 例子

- [决策树](/iris/dt.py)

## 2.6 异常检测 Anomaly Detection (无监督)

根据输入数据，对不符合预期模式的数据进行识别

- 概率密度函数：
- 高斯分布
- 数据均值
- 标准差

当维度大于一时，使用每个维度的高斯分布函数乘积 p(x) = p(x1) _ p(x2) _ ... p(xn)

### 2.6.1 例子

- [异常检测](/iris/noise.py)

## 2.7 主成分分析 PCA

### 数据降维

数据降维,是指在某些限定条件下,降低随机变量个数,得到到一组"不相关"主变量的过程。

作用:

- 减少模型分析数据量,提升处理效率,降低计算难度,
- 实现数据可视化。

PCA 是数据降维最常用的方法。

- 如何保留主要信息:投影后的不同特征数据尽可能分得开，即不相关

如何实现? 使投影后数据的方差最大,因为方差越大数据也越分散

计算过程:

- 原始数据预处理(标准化:u=0,σ=1)
- 计算协方差矩阵特征向量、及数据在各特征向量投影后的方差
- 根据需求(任务指定或方差比例)确定降维维度 k
- 选取 k 维特征向量,计算数据在其形成空间的投影

### PCA vs 线性回归

| **方面** | **PCA（主成分分析）** | **线性回归** |
| --- | --- | --- |
| **目标** | 降维，简化数据结构，保留最大方差信息。 | 预测，建立自变量与因变量之间的线性关系。 |
| **输入** | 高维数据矩阵 \(X\)。 | 自变量矩阵 \(X\) 和因变量向量 \(y\)。 |
| **输出** | 降维后的数据、主成分（特征向量）。 | 回归系数 \( \beta \)。 |
| **数学原理** | 特征值分解（Eigendecomposition）或奇异值分解（SVD）；基于协方差矩阵。 | 最小二乘法；最小化误差平方和 \( \| y - X\beta \|^2 \)。 |
| **求解方法** | 1. 数据标准化<br>2. 计算协方差矩阵<br>3. 特征分解<br>4. 选择主成分<br>5. 投影数据。 | 1. 模型设定 \( y = X\beta + \epsilon \)<br>2. 最小二乘法<br>3. 解析解 \( \beta = (X^TX)^{-1}X^Ty \)。 |
| **数据要求** | 需要对数据进行标准化（均值为 0，方差为 1），因为 PCA 对数据尺度敏感。 | 通常不需要标准化，但标准化可以提高数值稳定性。 |
| **应用场景** | 降维、特征提取、数据可视化、去除噪声、探索数据结构。 | 预测、因果分析、建立变量关系、回归分析。 |
| **对误差的处理** | 不涉及误差最小化，而是最大化数据的方差保留。 | 最小化预测值与实际值之间的误差平方和。 |
| **是否需要标签** | 无监督学习，不需要标签数据。 | 有监督学习，需要因变量 \( y \) 作为标签。 |

### 2.7.1 例子

- [异常检测](/iris/pca.py)
